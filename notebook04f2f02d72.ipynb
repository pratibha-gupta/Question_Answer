{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/uttarakhand/Uttarakhand.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.read_csv(\"/kaggle/input/uttarakhand/Uttarakhand.csv\")","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertForQuestionAnswering, BertTokenizer\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def answer_question(question, answer_text):\n    '''\n    This function will take a question_text string and an answer_text string (which contains the\n    answer), and identifies the words within the `answer_text` that are the\n    answer. And then print them out.\n    '''\n    # ======== Tokenizing ========\n    # Tokening the text string.\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = tokenizer.encode(question, answer_text)\n\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n\n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n\n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example through the model.\n    outputs = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                    token_type_ids=torch.tensor([segment_ids]), # The segment IDs to differentiate question from answer_text\n                    return_dict=True) \n\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n    # Start with the first token.\n    answer = tokens[answer_start]\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        \n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        # Otherwise, add a space then the token.\n        else:\n            answer += ' ' + tokens[i]\n    \n    answer = answer.replace('[CLS]','')\n    # A very un-professional way to deal with the [CLS] token\n    # which was being returned upon not finding the proper answer in the part of\n    # the dataset during the iteration\n    print('Answer: \"' + answer + '\"')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/uttarakhand/Uttarakhand.csv'\n# import pandas library\nimport pandas as pd\nimport re\n\n#function to remove emojis and other possible icons\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing csv file of uttarakhan dataset\nimport csv\ndata = ''\n\nwith open(path, 'r') as file:\n    reader = csv.reader(file)\n    for row in reader:\n        deEmojifiedText = deEmojify(str(row[0]))\n        data+=deEmojifiedText+'. '\n\nprint(row)\nprint(len(data))\nprint(data[:100])","execution_count":6,"outputs":[{"output_type":"stream","text":"['Sorry guys, I will not be able to tweet today, saddened by the bursting of the glacier in #Uttarakhand.\\nभगवान बद्री… https://t.co/osVaStOfuG']\n838297\ntweet. Horrible news out of #Uttarakhand. Prayer for #Uttarakhand . Ohhh God Please Save #Uttarakhan\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wrapping all the text.\nimport textwrap\n\nwrapper = textwrap.TextWrapper(width=80) \nprint(wrapper.fill(data[1701:3300]))","execution_count":7,"outputs":[{"output_type":"stream","text":"loods in Uttarakhand due to Glacial burst is so scary. I hope all are safe. I\npray for speedy return to normalcy a… https://t.co/1t01RaAYFL. Prayers for\n#Uttarakhand  My State #Uttarakhand. Respected indians  Let pray for the  people\nand provide all the support we can let's help the people around. #Uttarakhand.\nLet's pray for Uttarakhand.  Eventually it will reach the people in #Chamoli\n#Uttarakhand. Hope ppl in safe if you're stuck or anywhere near the affected\narea of flash flood please contact Disaster Operatio… https://t.co/cywjzpswTh.\nYou know it...when you see these response so soon. @HMOIndia what is the\nreason???? #Uttarakhand https://t.co/MvVT16zOW5. 4 Army columns, Two Medical\nteams,  one Engineering Task Force deployed at Ringi village. Army helicopters\non aeria… https://t.co/Y1tGCRIOeZ. The more hydro-power plants u build, the more\nentropy (floods) you'll receive..☺ #Uttarakhand #ClimateAction…\nhttps://t.co/AbXRVMAlcI. My prayers are with the people of Uttarakhand May\nMahadev protect them all❤ #Uttarakhand.. The tragedy Uttarakhand is facing is\nbeyond imagination. #Uttarakhand. Prayers with #Uttarakhand !. Extremely\nsaddened to know about the massive flooding at #Uttarakhand,   May Allah protect\neveryone.  Along with he… https://t.co/ML4XXemuU7. #BJP president #JPNadda\nspeaks to #Uttarakhand CM #TSRawat and enquires about the flood situation in\n#Chamoli. He a… https://t.co/mC3XVY7DiH. Really Really praying for all in\n#Uttarakhand.. https://t.co/98sYpnqyMg. Stay safe ppl of  #Uttarakhand\nhttps://t.co/QBcUhR8Fui. #Uttarakhand God please save people. Pray #Uttar\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying some questions to answer them.\nquestion = \"What is the helpline number?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1500):((i+1)*1500)])","execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tokenizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4e811eb950b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What is the helpline number?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-77ff63ea0b06>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, answer_text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Tokening the text string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Apply the tokenizer to the input text, treating them as a text-pair.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Report how long the input sequence is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"what happened in uttarakhand?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1500):((i+1)*1500)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"how much water level rose in rishikesh?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1500):((i+1)*1500)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"what is the emergency helpline number?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1500):((i+1)*1500)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"How many army units were deployed?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1500):((i+1)*1500)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question = \"How many people are affected?\"\nfor i in range(0,10):\n  answer_question(question, data[(i*1600):((i+1)*1600)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}